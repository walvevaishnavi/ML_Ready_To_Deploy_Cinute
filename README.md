ğŸš€ Ready-to-Deploy Machine Learning Collection(Read-Only)

A polished collection of five production-ready ML notebooks and scripts â€” each built with reproducibility, clarity, and deployment in mind.

ğŸ”­ Projects included

Each notebook is a stand-alone, end-to-end mini-project: data ingestion â†’ clean preprocessing â†’ EDA (static + interactive) â†’ model building â†’ evaluation â†’ model export for deployment.

Insurance.ipynb â€” Claims / premium modelling pipeline: feature engineering for categorical policy data, regression/classification baselines, model explainability notes, and a serialized model ready for API serving.

Salary_Distribution.ipynb â€” HR/compensation analysis: distributional analysis, outlier handling, and predictive salary-range estimation using regression ensembles.

Smart home device usage.ipynb â€” Time-series / usage-pattern analysis for IoT devices: sessionization, feature windows, visualization with Plotly, and anomaly-detection prototype.

Solar power gen.ipynb â€” Renewable energy forecasting: weather-feature fusion, lag features, model pipeline for short-term generation forecasting, and notes for edge/cloud deployment.

Wine data set.ipynb â€” Classic classification workflow: data cleaning, feature importance, classifier comparison, cross-validation, and model export with reproducible scoring.

âœ¨ Why this repo is different

Deployment-first: notebooks include model serialization (.pkl/joblib) and a clear path to wrap models into Flask/FastAPI or Streamlit apps.

Interactive + static EDA: Matplotlib/Seaborn for publication-ready charts and Plotly for interactive dashboards.

Modular pipelines: code separated into preprocessing, feature engineering, modelling, and utility blocks â€” easy to refactor into src/ modules.

Reproducible experiments: consistent random seeds, requirements.txt, and notebook checkpoints showing metric baselines.

Practical notes: pointers for hyperparameter tuning, evaluation choices, and deployment considerations (containerization, cloud hints).

ğŸ§° Tech stack

Python 3.8+ â€¢ Pandas, NumPy, Matplotlib, Seaborn, Plotly, Scikit-Learn, joblib
ğŸ Quickstart
# 1. clone
git clone <repo-url>
cd repo-name


# 2. create venv and install
python -m venv venv
source venv/bin/activate # mac/linux
venv\Scripts\activate # windows
pip install -r requirements.txt


# 3. open notebooks
jupyter lab
# run the notebooks in order or use the exported scripts in `src/`
â”œâ”€â”€ data/ # raw and processed datasets
â”œâ”€â”€ notebooks/ # the five notebooks (Insurance, Salary..., etc.)
â”œâ”€â”€ src/ # reusable pipeline scripts (preprocess.py, model.py)
â”œâ”€â”€ models/ # exported trained models (.pkl / .joblib)
â”œâ”€â”€ reports/ # visuals, metric summaries, model cards
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile # optional: containerize the API
â””â”€â”€ README.md
